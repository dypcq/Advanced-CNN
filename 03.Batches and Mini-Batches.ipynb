{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training a neural network, we feed the training data to our network. <br>\n",
    "Each full scan of the training data is called an **epoch**. <br>\n",
    "If we feed all of the training data in one step, we call it **batch mode** <font color=\"blue\">(the batch size equals the size of the training set)</font>. <br> \n",
    "However, in most cases, we divide the training data into smaller subsets while feeding the data to our model, just as in other machine learning algorithms. This is called **mini-batch** mode. <br>\n",
    "Sometimes, we are forced to do this because the complete training set is too big and doesn't fit in the memory. If we look at the training time, we would say: the bigger the batch size, the better (as long as the batch fits in the memory). <br><br>\n",
    "However, using mini-batches also has other advantages.<br>\n",
    "**Firstly**, it reduces the complexity of the training process.<br> \n",
    "**Secondly**, it reduces the effect of noise on the model by summing or averaging the gradient (reducing the variance).\n",
    "<br><br>\n",
    "**In mini-batch mode, the optimizer uses a balance of efficiency and robustness.**\n",
    "\n",
    "If the size of the mini-batch–also called batch size–is too small, the model can converge faster but is more likely to pick up noise. <br>\n",
    "If the batch size is too large, the model will probably converge slower but the estimates of error gradients will be more accurate.<br> \n",
    "<font color=\"red\"> **Note:** </font> For deep learning, when dealing with a lot of data, it's good to use large batch sizes. These batches are also great for parallelization on GPUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=False,\n",
    "                                   validation_split=0.25)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('data',\n",
    "                                                target_size = (150,150),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset = \"training\")\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory('data',\n",
    "                                            target_size = (150,150),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'categorical',\n",
    "                                            subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape = (150, 150,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set our callback functions. We use early stopping to prevent **overfitting**, ModelCheckpoint to save our best model automatically, and TensorBoard for an analysis of our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', patience=5, verbose=2),\n",
    "             ModelCheckpoint('checkpoints/weights.{epoch:02d}-'+str(batch_size)+'.hdf5',\n",
    "                             save_best_only=True),  TensorBoard()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training our first model with a batch size of **32**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 50\n",
    "history_32 = model.fit_generator(\n",
    "            training_set,\n",
    "            epochs = n_epochs,\n",
    "            verbose = 1,\n",
    "            validation_data = validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we recompile our model to make sure the weights are initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training our model with a batch size of **256**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 50\n",
    "history_256 = model.fit_generator(\n",
    "            training_set,\n",
    "            epochs = n_epochs,\n",
    "            verbose = 1,\n",
    "            validation_data = validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_256 = history_256.history['val_acc']\n",
    "val_acc_32 = history_32.history['val_acc']\n",
    "plt.plot(range(len(val_acc_32)), val_acc_32, label='CNN model with 32 BN')\n",
    "plt.plot(range(len(val_acc_256)), val_acc_256, label='CNN model with 256 BN')\n",
    "plt.title('Validation accuracy on dataset')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(val_acc_256), max(val_acc_32))\n",
    "print(len(val_acc_256), len(val_acc_32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> **Note:** </font><br>\n",
    "when using a bigger batch size, we need more epochs (in theory, the total number of steps/updates for converging should be the same regardless of the batch size). However, more interestingly, the validation accuracy of the model with a batch size of 256 is a bit higher. Whereas the model with a batch size of 32 tops. <br><br>\n",
    "When we train our model with a smaller batch size, the model might pick up a bit more noise. However, by fine-tuning our model further (for example, increasing the patience when using early stopping and decreasing the learning rate), the model with a batch size of 32 should be able to achieve similar accuracies. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
